<!-- <!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>AMAF-Net: Adaptive Multi-modal Alignment Framework</title>
  <link rel="stylesheet" href="style.css">
</head>

<body>

<div class="container">

  <h1>Training-Free Multil-Modal Alignment for Fine-Grained Couterfeit Fruit Detection</h1>
  <p class="affiliation">
  Quynh Nguyen Huu<sup>1*</sup>, Dat Tran-Anh<sup>2*</sup>, Thieu Huy Nguyen<sup>2</sup>, Ngoc Anh Nguyen Thi<sup>1</sup>, Nguyen Huu Gia Bach<sup>3</sup><br>
  <br>
  <sup>1</sup>CMC University, Hanoi, Vietnam<br>
  <sup>2</sup>Faculty of Information Technology, Thuyloi University, Hanoi, Vietnam<br>
  <sup>3</sup>Yen Hoa High School, Hanoi, Vietnam<br>
  <br>
  *Corresponding author
  </p>

  <h3 class="conference">International Conference on Artificial Intelligence: Impacts and Potentials 2025</h3>

  <div class="links">
    <a href="ICAI-IP2025-paper-final.pdf">üìÑ Full Paper</a>
    <a href="ICAI-IP_POSTER_75.pdf">üñºÔ∏è Poster</a>
  </div>

  <h2>Abstract</h2>
  <p class="abstract">
    Fine-grained counterfeit detection remains a challenge due to subtle visual differences, limited samples, and modality inconsistencies between vision and language. We introduce AMAF-Net, an Adaptive Multi-modal Alignment Framework designed for training-free fine-grained recognition. It enhances textual semantics through attribute-guided prompts generated by large language models and refines visual prototypes via base knowledge anchoring. An adaptive fusion mechanism dynamically balances both modalities using confidence-based weighting complemented by lightweight metric refinement. Without retraining, AMAF-Net achieves 90.2% accuracy on the FG-Fruit dataset, surpassing existing vision‚Äìlanguage baselines by up to 5% and demonstrating strong efficiency and robustness for fine-grained counterfeit recognition.
  </p>

  <h2>Method Overview</h2>
  <img src="model.png" alt="method figure" class="figure">

  <h2>Key Results</h2>
  <ul>
    <li>Highest overall accuracy across all benchmark sessions</li>
    <li>Low forgetting and stable performance curve</li>
    <li>Strong fine-grained recognition capability</li>
    <li>Effective for counterfeit fruit detection in real-world settings</li>
  </ul>

  <h2>BibTeX</h2>


</div>

</body>

<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>AMAF-Net: Training-Free Multi-Modal Alignment for Fine-Grained Counterfeit Fruit Detection</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link rel="stylesheet" href="style.css">
</head>

<body>
<div class="page-wrapper">

  <!-- Header -->
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>AMAF-Net: Adaptive Multi-modal Alignment Framework</title>
  <link rel="stylesheet" href="style.css">
</head>

<body>

<div class="container">

  <h1>Training-Free Multi-Modal Alignment for Fine-Grained Counterfeit Fruit Detection</h1>

  <p class="affiliation">
    Quynh Nguyen Huu<sup>1*</sup>, Dat Tran-Anh<sup>2*</sup>, Thieu Huy Nguyen<sup>2</sup>, 
    Ngoc Anh Nguyen Thi<sup>1</sup>, Nguyen Huu Gia Bach<sup>3</sup><br><br>
    <sup>1</sup> CMC University, Hanoi, Vietnam<br>
    <sup>2</sup> Faculty of Information Technology, Thuyloi University, Hanoi, Vietnam<br>
    <sup>3</sup> Yen Hoa High School, Hanoi, Vietnam<br><br>
    *Equal contribution &amp; corresponding authors
  </p>

  <p class="conference">
    <strong>Conference:</strong> International Conference on Artificial Intelligence: Impacts and Potentials 2025
  </p>

  <div class="links">
    <a href="ICAI-IP2025-paper-final.pdf">üìÑ Full Paper</a>
    <a href="ICAI-IP_POSTER_75.pdf">üñºÔ∏è Poster</a>
  </div>

  <!-- ABSTRACT -->
  <h2>Abstract</h2>
  <p class="abstract">
    Fine-grained counterfeit detection remains a challenge due to subtle visual differences, limited samples,
    and modality inconsistencies between vision and language. We introduce AMAF-Net, an Adaptive Multi-Modal
    Alignment Framework designed for training-free fine-grained recognition. It enhances textual semantics
    through attribute-guided prompts generated by large language models and refines visual prototypes via base
    knowledge anchoring. An adaptive fusion mechanism dynamically balances both modalities using confidence-based
    weighting complemented by lightweight metric refinement. Without any retraining, AMAF-Net achieves 90.2% accuracy
    on the FG-Fruit dataset, surpassing existing vision‚Äìlanguage baselines by up to 5% and demonstrating strong
    efficiency and robustness for fine-grained counterfeit recognition.
  </p>

  <!-- HIGHLIGHTS (M·ªöI TH√äM) -->
  <h2>Highlights</h2>
  <ul>
    <li><strong>Training-free:</strong> Kh√¥ng c·∫ßn fine-tune th√™m tr√™n downstream task.</li>
    <li><strong>Adaptive multi-modal fusion:</strong> K·∫øt h·ª£p linh ho·∫°t gi·ªØa ƒë·∫∑c tr∆∞ng ·∫£nh v√† text d·ª±a tr√™n ƒë·ªô tin c·∫≠y.</li>
    <li><strong>Fine-grained counterfeit detection:</strong> X·ª≠ l√Ω t·ªët c√°c c·∫∑p l·ªõp r·∫•t gi·ªëng nhau v·ªÅ th·ªã gi√°c.</li>
    <li><strong>Strong performance:</strong> ƒê·∫°t 90.2% accuracy tr√™n FG-Fruit, cao h∆°n c√°c VLM baseline t·ªõi kho·∫£ng 5%.</li>
  </ul>

  <!-- METHOD OVERVIEW -->
  <h2>Method Overview</h2>
  <p>
    AMAF-Net g·ªìm ba th√†nh ph·∫ßn ch√≠nh:
    (1) <strong>Text Enrichment</strong>: d√πng LLM ƒë·ªÉ sinh c√°c prompt gi√†u ng·ªØ nghƒ©a, nh·∫•n m·∫°nh thu·ªôc t√≠nh quan tr·ªçng c·ªßa t·ª´ng lo·∫°i qu·∫£;
    (2) <strong>Visual Prototype Refinement</strong>: t·∫≠n d·ª•ng ki·∫øn th·ª©c t·ª´ c√°c l·ªõp base ƒë·ªÉ tinh ch·ªânh prototype cho c√°c l·ªõp m·ªõi;
    (3) <strong>Adaptive Alignment &amp; Fusion</strong>: c√¢n b·∫±ng ƒë·ªông gi·ªØa score h√¨nh ·∫£nh v√† score t·ª´ m√¥ t·∫£ text, k·∫øt h·ª£p v·ªõi
    metric refinement nh·∫π ƒë·ªÉ ·ªïn ƒë·ªãnh d·ª± ƒëo√°n trong b·ªëi c·∫£nh few-shot.
  </p>
  <img src="model.png" alt="Overview of AMAF-Net" class="figure">

  <!-- KEY RESULTS -->
  <h2>Key Results</h2>
  <ul>
    <li>Highest overall accuracy across all benchmark sessions on the FG-Fruit dataset.</li>
    <li>Low forgetting and a stable performance curve across incremental sessions.</li>
    <li>Strong fine-grained recognition capability for visually similar fruit categories.</li>
    <li>Effective for counterfeit fruit detection in real-world settings with limited labeled data.</li>
  </ul>

  <!-- N·∫øu mu·ªën t√≥m t·∫Øt d·∫°ng s·ªë, c√≥ th·ªÉ gi·ªØ nguy√™n ul nh∆∞ tr√™n ƒë·ªÉ tr√°nh ph·∫£i ch·ªânh CSS -->

  <!-- BIBTEX -->
  <h2>BibTeX</h2>
  <pre>
@inproceedings{nguyen2025amafnet,
  title     = {Training-Free Multi-Modal Alignment for Fine-Grained Counterfeit Fruit Detection},
  author    = {Nguyen Huu, Quynh and Tran-Anh, Dat and Nguyen, Thieu Huy and Nguyen Thi, Ngoc Anh and Nguyen Huu, Gia Bach},
  booktitle = {Proceedings of the International Conference on Artificial Intelligence: Impacts and Potentials (ICAI-IP)},
  year      = {2025},
  address   = {Hanoi, Vietnam}
}
  </pre>

</div>

</body>
</html>
