<!-- <!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>AMAF-Net: Adaptive Multi-modal Alignment Framework</title>
  <link rel="stylesheet" href="style.css">
</head>

<body>

<div class="container">

  <h1>Training-Free Multil-Modal Alignment for Fine-Grained Couterfeit Fruit Detection</h1>
  <p class="affiliation">
  Quynh Nguyen Huu<sup>1*</sup>, Dat Tran-Anh<sup>2*</sup>, Thieu Huy Nguyen<sup>2</sup>, Ngoc Anh Nguyen Thi<sup>1</sup>, Nguyen Huu Gia Bach<sup>3</sup><br>
  <br>
  <sup>1</sup>CMC University, Hanoi, Vietnam<br>
  <sup>2</sup>Faculty of Information Technology, Thuyloi University, Hanoi, Vietnam<br>
  <sup>3</sup>Yen Hoa High School, Hanoi, Vietnam<br>
  <br>
  *Corresponding author
  </p>

  <h3 class="conference">International Conference on Artificial Intelligence: Impacts and Potentials 2025</h3>

  <div class="links">
    <a href="ICAI-IP2025-paper-final.pdf">üìÑ Full Paper</a>
    <a href="ICAI-IP_POSTER_75.pdf">üñºÔ∏è Poster</a>
  </div>

  <h2>Abstract</h2>
  <p class="abstract">
    Fine-grained counterfeit detection remains a challenge due to subtle visual differences, limited samples, and modality inconsistencies between vision and language. We introduce AMAF-Net, an Adaptive Multi-modal Alignment Framework designed for training-free fine-grained recognition. It enhances textual semantics through attribute-guided prompts generated by large language models and refines visual prototypes via base knowledge anchoring. An adaptive fusion mechanism dynamically balances both modalities using confidence-based weighting complemented by lightweight metric refinement. Without retraining, AMAF-Net achieves 90.2% accuracy on the FG-Fruit dataset, surpassing existing vision‚Äìlanguage baselines by up to 5% and demonstrating strong efficiency and robustness for fine-grained counterfeit recognition.
  </p>

  <h2>Method Overview</h2>
  <img src="model.png" alt="method figure" class="figure">

  <h2>Key Results</h2>
  <ul>
    <li>Highest overall accuracy across all benchmark sessions</li>
    <li>Low forgetting and stable performance curve</li>
    <li>Strong fine-grained recognition capability</li>
    <li>Effective for counterfeit fruit detection in real-world settings</li>
  </ul>

  <h2>BibTeX</h2>


</div>

</body>

<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>AMAF-Net: Training-Free Multi-Modal Alignment for Fine-Grained Counterfeit Fruit Detection</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link rel="stylesheet" href="style.css">
</head>

<body>
<div class="page-wrapper">

  <!-- Header -->
<header class="hero">
  <div class="hero-content">
    <p class="conference-pill">International Conference on Artificial Intelligence: Impacts and Potentials 2025</p>
    <h1>Training-Free Multi-Modal Alignment for Fine-Grained Counterfeit Fruit Detection</h1>
    <p class="subtitle">AMAF-Net: Adaptive Multi-Modal Alignment Framework</p>

    <p class="affiliation">
      Quynh Nguyen Huu<sup>1*</sup>, Dat Tran-Anh<sup>2*</sup>, Thieu Huy Nguyen<sup>2</sup>, 
      Ngoc Anh Nguyen Thi<sup>1</sup>, Nguyen Huu Gia Bach<sup>3</sup><br><br>
      <sup>1</sup>CMC University, Hanoi, Vietnam<br>
      <sup>2</sup>Faculty of Information Technology, Thuyloi University, Hanoi, Vietnam<br>
      <sup>3</sup>Yen Hoa High School, Hanoi, Vietnam<br><br>
      *Equal contribution &amp; corresponding authors
    </p>

    <div class="links">
      <a class="btn primary" href="ICAI-IP2025-paper-final.pdf">üìÑ Full Paper</a>
      <a class="btn secondary" href="ICAI-IP_POSTER_75.pdf">üñºÔ∏è Poster</a>
    </div>
  </div>
</header>

<!-- Main content -->
<main class="container">

  <!-- Highlights -->
  <section class="section">
    <h2>Highlights</h2>
    <ul class="highlights">
      <li><strong>Training-free:</strong> No additional fine-tuning required for downstream tasks.</li>
      <li><strong>Adaptive multi-modal fusion:</strong> Confidence-based weighting between visual and textual cues.</li>
      <li><strong>Fine-grained counterfeit detection:</strong> Robust under subtle visual differences and class ambiguity.</li>
      <li><strong>Strong performance:</strong> Achieves 90.2% accuracy on FG-Fruit, improving up to 5% over prior VLM baselines.</li>
    </ul>
  </section>

  <!-- Abstract -->
  <section class="section">
    <h2>Abstract</h2>
    <p class="abstract">
      Fine-grained counterfeit detection remains a challenge due to subtle visual differences, limited samples, 
      and modality inconsistencies between vision and language. We introduce AMAF-Net, an Adaptive Multi-modal 
      Alignment Framework designed for training-free fine-grained recognition. It enhances textual semantics 
      through attribute-guided prompts generated by large language models and refines visual prototypes via base 
      knowledge anchoring. An adaptive fusion mechanism dynamically balances both modalities using confidence-based 
      weighting complemented by lightweight metric refinement. Without retraining, AMAF-Net achieves 90.2% accuracy 
      on the FG-Fruit dataset, surpassing existing vision‚Äìlanguage baselines by up to 5% and demonstrating strong 
      efficiency and robustness for fine-grained counterfeit recognition.
    </p>
  </section>

  <!-- Method Overview -->
  <section class="section">
    <h2>Method Overview</h2>
    <figure class="figure-wrapper">
      <img src="model.png" alt="Overview of the AMAF-Net framework" class="figure">
      <figcaption class="caption">
        Figure 1: Overview of AMAF-Net. Large language models enrich attribute-guided textual prompts, 
        while base visual knowledge refines prototypes. An adaptive fusion module combines both modalities 
        with confidence-based weighting and lightweight metric refinement.
      </figcaption>
    </figure>
  </section>

  <!-- Key Results -->
  <section class="section">
    <h2>Key Results</h2>
    <ul class="key-results">
      <li>Highest overall accuracy across all benchmark sessions on FG-Fruit.</li>
      <li>Low forgetting and stable performance across incremental sessions.</li>
      <li>Strong fine-grained recognition ability for visually similar fruit categories.</li>
      <li>Effective in real-world counterfeit detection scenarios with limited labeled data.</li>
    </ul>

    <!-- Optional simple table -->
    <div class="results-table-wrapper">
      <table class="results-table">
        <thead>
          <tr>
            <th>Method</th>
            <th>Type</th>
            <th>Accuracy (%)</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>CLIP baseline</td>
            <td>Vision‚ÄìLanguage</td>
            <td>~85.0</td>
          </tr>
          <tr>
            <td>Best prior VLM</td>
            <td>Vision‚ÄìLanguage</td>
            <td>~86‚Äì88</td>
          </tr>
          <tr class="highlight-row">
            <td><strong>AMAF-Net (ours)</strong></td>
            <td><strong>Training-free VLM</strong></td>
            <td><strong>90.2</strong></td>
          </tr>
        </tbody>
      </table>
      <p class="table-note">Note: Values are illustrative summaries; please refer to the full paper for detailed metrics.</p>
    </div>
  </section>

  <!-- BibTeX -->
  <section class="section">
    <h2>BibTeX</h2>
    <p class="bibtex-note">If you find this work useful, please cite:</p>
    <pre class="code-block">
@inproceedings{nguyen2025amafnet,
title     = {Training-Free Multi-Modal Alignment for Fine-Grained Counterfeit Fruit Detection},
author    = {Nguyen Huu, Quynh and Tran-Anh, Dat and Nguyen, Thieu Huy and Nguyen Thi, Ngoc Anh and Nguyen Huu, Gia Bach},
booktitle = {Proceedings of the International Conference on Artificial Intelligence: Impacts and Potentials (ICAI-IP)},
year      = {2025},
address   = {Hanoi, Vietnam}
}
    </pre>
  </section>

</main>

<!-- Footer -->
<footer class="footer">
  <p>¬© 2025 AMAF-Net Authors. All rights reserved.</p>
</footer>

</div>
</body>
</html>

</html>
-->
