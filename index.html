<!-- <!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>AMAF-Net: Adaptive Multi-modal Alignment Framework</title>
  <link rel="stylesheet" href="style.css">
</head>

<body>

<div class="container">

  <h1>Training-Free Multil-Modal Alignment for Fine-Grained Couterfeit Fruit Detection</h1>
  <p class="affiliation">
  Quynh Nguyen Huu<sup>1*</sup>, Dat Tran-Anh<sup>2*</sup>, Thieu Huy Nguyen<sup>2</sup>, Ngoc Anh Nguyen Thi<sup>1</sup>, Nguyen Huu Gia Bach<sup>3</sup><br>
  <br>
  <sup>1</sup>CMC University, Hanoi, Vietnam<br>
  <sup>2</sup>Faculty of Information Technology, Thuyloi University, Hanoi, Vietnam<br>
  <sup>3</sup>Yen Hoa High School, Hanoi, Vietnam<br>
  <br>
  *Corresponding author
  </p>

  <h3 class="conference">International Conference on Artificial Intelligence: Impacts and Potentials 2025</h3>

  <div class="links">
    <a href="ICAI-IP2025-paper-final.pdf">üìÑ Full Paper</a>
    <a href="ICAI-IP_POSTER_75.pdf">üñºÔ∏è Poster</a>
  </div>

  <h2>Abstract</h2>
  <p class="abstract">
    Fine-grained counterfeit detection remains a challenge due to subtle visual differences, limited samples, and modality inconsistencies between vision and language. We introduce AMAF-Net, an Adaptive Multi-modal Alignment Framework designed for training-free fine-grained recognition. It enhances textual semantics through attribute-guided prompts generated by large language models and refines visual prototypes via base knowledge anchoring. An adaptive fusion mechanism dynamically balances both modalities using confidence-based weighting complemented by lightweight metric refinement. Without retraining, AMAF-Net achieves 90.2% accuracy on the FG-Fruit dataset, surpassing existing vision‚Äìlanguage baselines by up to 5% and demonstrating strong efficiency and robustness for fine-grained counterfeit recognition.
  </p>

  <h2>Method Overview</h2>
  <img src="model.png" alt="method figure" class="figure">

  <h2>Key Results</h2>
  <ul>
    <li>Highest overall accuracy across all benchmark sessions</li>
    <li>Low forgetting and stable performance curve</li>
    <li>Strong fine-grained recognition capability</li>
    <li>Effective for counterfeit fruit detection in real-world settings</li>
  </ul>

  <h2>BibTeX</h2>


</div>

</body>

<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>AMAF-Net: Training-Free Multi-Modal Alignment for Fine-Grained Counterfeit Fruit Detection</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link rel="stylesheet" href="style.css">
</head>

<body>
<div class="page-wrapper">

  <!-- Header -->
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>AMAF-Net: Adaptive Multi-modal Alignment Framework</title>
  <link rel="stylesheet" href="style.css">
</head>

<body>

<div class="container">

  <h1>Training-Free Multi-Modal Alignment for Fine-Grained Counterfeit Fruit Detection</h1>

  <p class="affiliation">
    Quynh Nguyen Huu<sup>1*</sup>, Dat Tran-Anh<sup>2*</sup>, Thieu Huy Nguyen<sup>2</sup>, 
    Ngoc Anh Nguyen Thi<sup>1</sup>, Nguyen Huu Gia Bach<sup>3</sup><br><br>
    <sup>1</sup> CMC University, Hanoi, Vietnam<br>
    <sup>2</sup> Faculty of Information Technology, Thuyloi University, Hanoi, Vietnam<br>
    <sup>3</sup> Yen Hoa High School, Hanoi, Vietnam<br><br>
    *Corresponding authors
  </p>

  <p class="conference">
    <strong>Conference:</strong>
    <span style="color:red; font-weight:bold;">
      International Conference on Artificial Intelligence: Impacts and Potentials 2025
    </span>
  </p>

  <div class="links">
    <a href="ICAI-IP2025-paper-final.pdf">üìÑ Full Paper</a>
    <a href="ICAI-IP_POSTER_75.pdf">üñºÔ∏è Poster</a>
  </div>

  <!-- ABSTRACT -->
  <h2>Abstract</h2>
  <p class="abstract">
    Fine-grained counterfeit detection remains challenging due to subtle inter-class differences,
    limited samples, and inconsistencies between visual and textual modalities. We introduce AMAF-Net,
    an Adaptive Multi-Modal Alignment Framework designed for training-free fine-grained recognition.
    AMAF-Net enriches textual semantics using attribute-guided prompts generated by large language models
    and refines visual prototypes through base-knowledge anchoring. An adaptive fusion mechanism then
    balances the two modalities using confidence-based weighting, complemented by lightweight metric
    refinement. Without any retraining, AMAF-Net achieves 90.2% accuracy on the FG-Fruit dataset,
    surpassing existing vision‚Äìlanguage baselines by up to 5% and demonstrating strong robustness in
    counterfeit fruit detection.
  </p>

  <!-- HIGHLIGHTS -->
  <h2>Highlights</h2>
  <ul style="margin-left:20px">
    <li><strong>Training-free:</strong> No fine-tuning required for downstream recognition tasks.</li>
    <li><strong>Adaptive multi-modal fusion:</strong> Dynamically combines visual and textual cues based on confidence weighting.</li>
    <li><strong>Fine-grained counterfeit detection:</strong> Handles visually similar fruit categories with high precision.</li>
    <li><strong>Strong performance:</strong> Achieves 90.2% accuracy on FG-Fruit, outperforming VLM baselines by up to 5%.</li>
  </ul>

  <!-- METHOD OVERVIEW -->
  <h2>Method Overview</h2>
  <p>
    AMAF-Net consists of three key components that operate in a training-free pipeline:
  </p>

  <ul style="margin-left:20px">
    <li><strong>Text Enrichment:</strong> Large language models generate attribute-enhanced prompts capturing fine-grained semantics.</li>
    <li><strong>Visual Prototype Refinement:</strong> Visual prototypes are refined by leveraging base-class knowledge for better alignment.</li>
    <li><strong>Adaptive Alignment & Fusion:</strong> Final predictions combine image-based and text-based similarities through confidence-driven weighting, with lightweight metric refinement for stability.</li>
  </ul>

  <img src="model.png" alt="Overview of the AMAF-Net framework" class="figure">

  <!-- KEY RESULTS -->
  <h2>Key Results</h2>
  <ul style="margin-left:20px">
    <li>AMAF-Net achieves the highest overall accuracy on the FG-Fruit benchmark.</li>
    <li>Low forgetting and stable performance across incremental sessions.</li>
    <li>Strong ability to distinguish visually similar fruit categories, including counterfeit vs. genuine pairs.</li>
    <li>Effective in real-world counterfeit detection scenarios with limited annotations.</li>
  </ul>

  <!-- BIBTEX -->
  <h2>BibTeX</h2>
  <pre>
@inproceedings{nguyen2025amafnet,
  title     = {Training-Free Multi-Modal Alignment for Fine-Grained Counterfeit Fruit Detection},
  author    = {Nguyen Huu, Quynh and Tran-Anh, Dat and Nguyen, Thieu Huy and Nguyen Thi, Ngoc Anh and Nguyen Huu, Gia Bach},
  booktitle = {Proceedings of the International Conference on Artificial Intelligence: Impacts and Potentials (ICAI-IP)},
  year      = {2025},
  address   = {Hanoi, Vietnam}
}
  </pre>

</div>

</body>
</html>
